# MindRAG Environment Variables

# Hugging Face API Token
# Replace with your actual token or leave empty to use models without authentication
HUGGINGFACE_API_TOKEN=<paste_your_token_here>

# Model Configuration
# Set to "local" to use local models or "api" to use Hugging Face API
MODEL_TYPE=local
DEFAULT_MODEL=TheBloke/zephyr-7B-beta-GGUF
MODEL_BASENAME=zephyr-7b-beta.Q2_K.gguf

# Hardware Configuration
# Set to "cpu" or "cuda" based on your hardware
DEVICE='cuda'
# Force CUDA usage even if PyTorch doesn't detect it properly
FORCE_CUDA='true'
# Number of CPU threads to use for inference (when CPU is used)
CPU_THREADS='1'
# Number of GPU layers to offload (for LlamaCpp)
# For GTX 1650 with 4GB VRAM, 20-24 layers is a good balance
GPU_LAYERS='20'
# CUDA visible devices (leave empty to use all available GPUs)
CUDA_VISIBLE_DEVICES=0
# Force GPU usage in llama-cpp-python
LLAMA_CUBLAS='1'

# API Configuration
HOST=0.0.0.0
PORT=8000
EMBEDDINGS_DEVICE='cuda'
HYBRID_MODE='false'
